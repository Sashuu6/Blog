var store = [{
        "title": "Things to do after installing Fedora 28",
        "excerpt":"   Fedora has been my favourite from the day I started using Fedora. As Fedora was sponsored by Red Hat, I had to make sure I gave it a try. I started my linux life using Ubuntu. Switching from Ubuntu to Fedora was not a easy task. I found first few days of using Fedora a bit hard. So, after gaining some experience using Fedora, gave me an idea to write a blog about the initial steps to be performed by a newbie Fedora user or a user who has just installed Fedora. When I was planning to write the blog, came the release of Fedora 28.   These are the steps I followed after installing Fedora 28.   First, we will switch to superuser.   $ sudo su  There are few rpm packages to download. So,   $ cd Downloads  $ mkdir post-fedora-install $ cd post-fedora-install  Now, lets update the OS. If you are new to Fedora, the fedora’s update function will automatically update the repo list and will install the latest packages to the system.   $ dnf -y update   Lets start the SSH server. SSH will help to connect to your system remotely from another system in the same network.   $ systemctl start sshd  $ systemctl enable sshd   Lets install RPM fusion repository into the system. The fusion repository will help to install all free and non free 3rd party applications.   $ rpm -ivh https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-28.noarch.rpm  $ rpm -ivh https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-28.noarch.rpm  $ dnf -y update   If you are using GNOME, installing the GNOME tweak tool will help you customize the desktop environment.   $ dnf -y install gnome-tweak-tool   Lets install some media codecs to view all media files.   $ dnf -y install gstreamer-plugins-base gstreamer1-plugins-base gstreamer-plugins-bad gstreamer-plugins-ugly gstreamer1-plugins-ugly gstreamer-plugins-good-extras gstreamer1-plugins-good-extras gstreamer1-plugins-bad-freeworld ffmpeg gstreamer-ffmpeg    Lets install VLC.   $ dnf -y install vlc   Lets install Stacer. Stacer is an application to view system information and monitor all the system resources. As stacer is an open-source application, we must first download it from github.   $ wget https://github.com/oguzhaninan/Stacer/releases/download/v1.0.8/stacer-1.0.8_x64.rpm   Now lets install the downloaded rpm file.   $ dnf -y install stacer-1.0.8_x64.rpm   Lets install a torrent client. qbittorrent works just fine for Fedora 28.   $ dnf -y install qbittorrent   If you like to download videos from youtube. Then worry not, here comes youtube-dl. The youtube-dl is a commandline function to download youtube videos.   $ dnf -y install youtube-dl   Consider Fedora is your daily driver workstation. You might need to take screenshots or custom screenshots. Shutter snipping tools is the best alternative for Fedora.   $ dnf -y install shutter   Lets install unzip for uncompressing files.   $ dnf -y install unzip   Lets install java plugins for web browsers.   $ dnf -y install icedtea-web java-openjdk   Lets install java JDK.   $ yum install -y java-1.8.0-openjdk-devel   If you develop programs in C, you will need development tools.   $ dnf -y group install 'Development Tools'   Everyone has an IDE preference, I like to use Sublime. First we must import the sublime repo.   $ rpm -v --import https://download.sublimetext.com/sublimehq-rpm-pub.gpg   We then add the repo using config manager.   $ dnf config-manager --add-repo https://download.sublimetext.com/rpm/stable/x86_64/sublime-text.repo   Now, we are ready to install Sublime IDE.   $ dnf -y install sublime-text   If you are a gamer, you must have steam. For installing steam, we need to add the steam repo into the system.   $ dnf -y config-manager --add-repo=http://negativo17.org/repos/fedora-steam.repo   Now, lets install the update the repo list.   $ dnf -y update   Lets install Steam.   $ dnf -y install steam   This is it folks. These steps helped me make my Fedora be used as a daily driver.  ","categories": ["Linux","Fedora"],
        "tags": [],
        "url": "http://localhost:4000/linux/fedora/things-to-do-after-installing-fedora-28/",
        "teaser":"http://localhost:4000/assets/images/blog/things-to-do-after-installing-fedora-28.jpeg"},{
        "title": "Setup wake on LAN on local server",
        "excerpt":"   Why Wake on LAN? Wake on LAN is program to wake a system which is under deep sleep. That is, the system is shut down with power access. A system with an enabled Wake-on-LAN feature can be turned on using a special packet. This special packet is sent by the client remotely.   To implement Wake-On-LAN on the server, we need to install ethtool on our server and wakeonlan commanline program on our personal system.   First we will setup our server:-           Enable wake-on-LAN on BIOS settings. Enter into BIOS and search for wake on LAN and enable it. Make sure the ethernet cable connect from the server to the switch or router has all the 8 pins connected. This is because the magic packet is sent via a dedicatd pin, usually last 2 pins. Install ethtool into the server.       (Ubuntu / Debian)       $ sudo apt install ethtool           (Fedora)       $ sudo dnf install ethtool           (RHEL / CentOS)        $ sudo yum install ethtool                Now let’s find the name of ethernet port present in the system. This command will also reveal the mac address.       $ ip a           The above command will output the ethernet port name (Lets consider the ethernet name to be enp9s0).            Let’s check if ethtool detects the port.       $ sudo ethtool enp9s0                Now, let’s activate wake on lan.       $ sudo ethtool -s enp8s0 wol g                Now, we will setup wake on client on your system. i.e, the system that will be used to wake the server.       (Ubuntu / Debian)       $ sudo apt install etherwake           (Mac)       $ brew install wakeonlan           (Fedora / RHEL / CentOS)       $ wget http://gsd.di.uminho.pt/jpo/software/wakeonlan/downloads/wakeonlan-0.41-0.fdr.1.noarch.rpm  $ rpm -Uvh wakeonlan-0.41-0.fdr.1.noarch.rpm           Now, we can wake the system using the wakeonlan command. Execute the following command.      $ wakeonlan mac-address   The above command will wake your system.  ","categories": ["Linux","Networking"],
        "tags": [],
        "url": "http://localhost:4000/linux/networking/wake-on-lan-on-local-server/",
        "teaser":"http://localhost:4000/assets/images/blog/setup-wake-on-lan.jpg"},{
        "title": "Setup Localhost in MacOS",
        "excerpt":"   I first started developing commerial products using HTML, CSS, Javascript and many more languages that were used to code client-side of a web application. I first encountered server-side languages while having a technical conversation with one of my seniors. This conversation motivated to learn PHP. While surfing the internet, I found we couldn’t execute server-side languages like client-side. That is, I couldn’t see the output just by executing the code via a browser. To execute server-side languages like PHP, I had to establish a server inside my mac.   The following are the steps I used to setup a localhost on my mac. On mac, we dont have to install a server, it’s already pre-installed. We just have to enable it.      $ sudo apachectl start   To check whether the server is not running or not, go to http://localhost on the mac’s browser. PHP is also preinstalled on mac, we just have to enable it. First, we have to edit the server’s config file.      $ sudo nano /etc/apache2/httpd.conf   Now, we will search for PHP in this file using ctrl+w Remove # from the below line.      LoadModule php7_module libexec/apache2/libphp7.so   Restart apache server      $ sudo apachectl restart   We have a create a folder named “Sites” on the home directory.      $ sudo mkdir /Users/&lt;--your_username--&gt;/Sites   Lets save a file named index.php with the following contents.      &lt;?php echo \"Hello From Sites Folder!”; phpinfo();?&gt;    We have to add the Sites fold on the config file. Edit the file using the following command.      $ sudo nano /etc/apache2/httpd.conf   Inside the file, search for the word “library” using ctrl+w. Replace /Library/WebServer/Documents with /Users/&lt;–your_username–&gt;/Sites Now, we need a database and a DBMS for the localhost. Lets download mysql for mac. First we will download MySQL from this link. Install MySQL from dmg file. Now, we will configure MqSQL by opening MySQL from system preferences. Add a password and click legacy password. After this step, just click “start MySQL server”. You can use sql through bash by executing the following command.      $ /usr/local/mysql/bin/mysql -u root -p   Or you can use Sequel pro. Sequel pro is an application that can be used to any database remotely. That is, it can be used to connect to localhost and anyother server established on the network. Now, we will make the server communication encrypted. That is, we will enable SSL/HTTPS on localhost. Execute the following command to edit the config file.      $ sudo nano /etc/apache2/httpd.conf   Search for socache_shmcb_module using ctrl+w. Remove # from the following lines.      socache_shmcb_module libexec/apache2/mod_socache_shmcb.so   serach for LoadModule ssl_module libexec/apache2/mod_ssl.so and remove # from it.      LoadModule ssl_module libexec/apache2/mod_ssl.so   Delete # from Include /private/etc/apache2/extra/httpd-ssl.conf      /private/etc/apache2/extra/httpd-ssl.conf   We have to replace www.example.com:443 with localhost from the below file.      $ sudo nano /etc/apache2/extra/httpd-ssl.conf   Replace Library/WebServer/Documents. Replace that with /Users/&lt;–your_username–&gt;/Sites Add the following lines below &lt;VirtualHost_default_:443&gt;      &lt;Directory \"/Users/&lt;--your_username--&gt;/Sites\"&gt;      Options All      MultiviewsMatch Any      AllowOverride All      Require all granted     &lt;/Directory&gt;   Open the following config file.      $ sudo nano /etc/ssl/openssl.cnf   Add the following lines at the end.      [ san ]     subjectAltName                  = DNS:localhost   Now we generate a key using openSSL.      $ sudo openssl req -extensions san -config /etc/ssl/openssl.cnf -x509 -nodes -newkey rsa:4096 -keyout /private/etc/apache2/server.key -out /private/etc/apache2/server.crt -days 365 -subj \"/C=your_country/ST=your_state/L=your_city/O=hostname/CN=localhost\"   Now we will add the certificate using the following command.      $ sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /private/etc/apache2/server.crt   Now just restart the apache server.      $ sudo apachectl restart   We have not successfully established a server inside Mac.  ","categories": ["Linux","MacOS"],
        "tags": [],
        "url": "http://localhost:4000/linux/macos/setup-localhost-mac/",
        "teaser":"http://localhost:4000/assets/images/blog/localhost-on-mac.jpg"},{
        "title": "My Take On DevOps",
        "excerpt":"   What is DevOps?   I heard this term first when I was doing my internship at Appfabs. They focused on DevSecOps as they were a Security based company. So, to understand DevSecOps, I had to know what is DevOps. The first thing I was told that DevOps is a methodology, not a technology. DevOps is a methodology used in SDLC (Software Development Life Cycle) where the development team and operations team work together from designing stage to production support. To understand DevOps and why DevOps was introduced, we need to know the methods used before DevOps was introduced.   All these started when the waterfall model was introduced into SDLC. This model was best suited when:-     The product requirements are clear and fixed.   The product definition is stable. That is, the product’s client has a clear cut idea on how the product must be.   The project has very least risk during development.   The main disadvantages using this method was:-     It was slow.   Change implement was slow.   As in software field, slow means loss. Using this method in huge and sensitive projects gave companies huge losses. Keeping the disadvantages in mind, a new model was introduced that could fix all the disadvantages of the waterfall model. Thus, came the Agile methodology. Agile methodology brought continuous iteration in product development and testing throughout the SDLC.   The Agile methodology is bested when:-     The product requirements frequently change.   The product requires faster development.   And guess what? even this method didn’t work out for many companies, due to:-     Lack of agility in the operations phase of the product lifecycle.   The issue in the code environment. That is, the code works perfectly fine in developer’s PC, but won’t work on the production environment (Server).   The deployment of the product is linear.   To solve the disadvantages in agile methodology, the DevOps was introduced. The DevOps was best suited when:-     The product’s requirement changed frequently.   When the Operations team needs to become agile.   For bridging the gap between the development team and the operations team.   To understand DevOps better, we will learn a case study of a big shot company like Facebook who finally used DevOps due to their product’s need. In 2011, Facebook introduced a major update that includes many cool features like timeline support, music support and many more. The public was notified about this new release. As Facebook as a huge hit during that time, all the active Facebook users (almost 500 million) started using Facebook to use these features. As Facebook’s servers during that time were not prepared to handle this huge traffic, it got shut down causing a server meltdown. Added to this major issue, the new features got a mixed opinion among the users. That is, many users loved it, other users absolutely hated it and few requested for modification. These kinds of reaction created a chaotic situation in Facebook headquarters. To solve this issue, they brought in DevOps and introduced a technique called “Dark Launching”. The “Dark Launching” is a type of product release technique in which:-     New features will be first released to a smaller group of users (mainly internal BETA testers). There will be constant communication between the team and the BETA testers. The communication involves taking feedback and other techniques that help to improve the features to the user’s need.   When the team gets an assurance that the features can be released to the public, they release it to public BETA and then to the public through multiple product releases.      Facebook achieves “Dark Launching” via the use of the delivery pipeline. A delivery pipeline is a set of steps that a code goes through till it reached production. A pipeline contains an automated building, testing and deploying tools that work together till the product’s release.   So after learning a case, we can say that “DevOps is a software development approach that involves continuous development, testing, integration, deployment and monitoring throughout the development cycle.”   DevOps Lifecycles   The lifecycle represents the stages in DevOps:-     Continuous Development: The continuous development involves the product’s development is split into smaller development cycles.   Continuous Testing: This cycle involves the Quality Assurance team identifying any bugs or errors in the code after every development cycle.   Continuous Integration: This cycle involves integrating the new code with the existing code present in the production environment.   Continuous Deployment: This cycle involves the deployment of code into the environment. The deployment is done in such a way that, it doesn’t affect the product’s current network traffic.   Continuous Monitoring: The monitoring helps the team to monitor changes in the application after every lifecycle.   DevOps Tools      The main advantage that DevOps gave us is “Automation”. The developer can automate the whole operations process in the software lifecycle. Automation in DevOps is achieved using tools at different stages of its cycle. The following tools (that I have used) are used at different stages:-     Plan - Jira   Code - Git   Build - Gradle, Maven   Test - Selenium, JUnit   Integrate - Jenkins   Deploy - Docker   Operate - Ansible   Monitor - Nagios, slack   Let’s consider an example of a company that has implemented DevOps methodology in their product and have used few tools for automation. The tool Jenkins (integration tool) detects changes in code and automatically triggers a build using tools like Gradle or Maven. After the build, Jenkins will deploy the code for testing. The testing is done by test tools like Selenium (if web app), JUnit (For Java apps) or any other testing tool. The deployment is handled using Docker. The running code is then monitored by tools like Nagios, Splunk or ELK Slack.   ","categories": ["DevOps"],
        "tags": [],
        "url": "http://localhost:4000/devops/my-take-on-devops/",
        "teaser":"http://localhost:4000/assets/images/blog/my-take-on-devops/devops-cover.png"},{
        "title": "Things to do after installing RHEL 7",
        "excerpt":"   The RHEL is RedHat Enterprise Linux,  a server distribution of Linux. RHEL is not like other Linux distribution because you have to pay money for the company’s services. But, the Operating System is free to use. The latest version of RHEL is RHEL 7. The following things where the first things I did, after install RHEL 7 on my virtual machine. First, we have to do is enable RedHat subscription. I subscribed for a developer account through the RedHat website and developer subscription is free. Our machine should be registered for doing anything. This registration can be done using a package called subscription manager. Follow the steps to register your machine.   $ subscription-manager register --username your_username --password your_password --auto-attach $ subscription-manager list --available $ subscription-manager list $ subscription-manager service-level --list $ subscription-manager service-level --set=self-support   Now, we need to enable yum repositories to install any applications via the repo list. The following steps will enable yum repositories.   $ subscription-manager repos --list $ yum repolist all $ yum repolist $ vi /etc/yum.repos.d/redhat.repo  Uncomment all the repositories you need for your system.   $ yum repolist all  Let’s update your RHEL 7 system.   $ yum update   Now, we need to change the system’s hostname. If you have done it during your RHEL 7 installation, then this step is not necessary.   $ echo hostname $ nano /etc/hostname   Change the name to any name and save it. Just reboot the system to see the changes in your hostname. We will now install an interesting application, this application can be used to view websites via the terminal. The view won’t be appealing but can be used for basic needs. To install links, type the following command.   $ yum install links   Now we will set up a local server in your system. To do that, we will have to install Apache server, PHP and MySQL. I am using PHP 5.6 because many of the functions that I used got deprecated after PHP 5.6 like mcrypt and many more.  Let’s install the Apache server.   $ yum install httpd   Now, we have to open port in the firewall for the HTTPD in the server.   $ firewall-cmd --add-service=http $ firewall-cmd --permanent --add-port=80/tcp $ firewall-cmd --reload   Now, we will restart and enable HTTPD service.   $ systemctl restart httpd.service $ systemctl start httpd.service $ systemctl enable httpd.service   Now let’s check if the server is up.   $ links &lt;your_ip_address&gt;   If you see apache server page, then your server is up and running. Now, we will enable indexing in the server.  $ nano /etc/httpd/conf.d/welcome.conf   In the file, search for set -indexes and change it to set +indexes. If it is commented, then uncomment it by removing # from it.   Now we will restart apache to apply the changes.   $ systemctl restart httpd   Now we will install PHP 5.6.   $ yum search php $ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm $ rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm $ yum install -y php56w php56w-opcache php56w-xml php56w-mcrypt php56w-gd php56w-devel php56w-mysql php56w-intl php56w-mbstring   We will check, if PHP is working on our system or not.   $ echo -e \"&lt;?php\\nphpinfo();\\n?&gt;\" &gt;&gt; /var/www/html/phpinfo.php   To check if the php file is working, just type:-   $ links http://&lt;ip_address&gt;/phpinfo.php   Now, we will install mariaDB.   $ yum install mariadb-server mariadb   Now, we will start mariaDB service.   $ systemctl start mariadb.service $ systemctl enable mariadb.service   To access the database via the terminal (I dont prefer phpmyadmin).   $ mysql -u root -p   Now, lets install gcc (C++ compiler) in the system.   $ yum install gcc  Now, let’s install java.   $ yum install java   Now, we will install Apache Tomcat.   $ yum install tomcat   Now, let’s start tomcat.   $ systemctl start tomcat $ systemctl enable tomcat.service   We have to open a port on our firewall. To do that:-   $ firewall-cmd --zone=public --add-port=8080/tcp --permanent $ firewall-cmd --reload   Now, let’s install nmap. The nmap is an open-source security scanner.   $ yum install nmap   To download files from the internet, we use wget. We will now install wget.   $ yum install wget   To unzip files, we need unzip. To install unzip, do the following:-   $ yum install unzip   Now, we will install an exploit and backdoor scanner called rootkit hunter.   $ yum install rkhunter   To start the scanner, just type.   $ rkhunter --check   When you have a server, you need to have a web panel. We will install a Webmin for satisfying that purpose.   $ wget http://prdownloads.sourceforge.net/webadmin/webmin-1.740-1.noarch.rpm $ rpm -ivh webmin-1.740-1.noarch.rpm   Now, we have to open the port for accessing it remotely via a web browser.   $ firewall-cmd --zone=public --add-port=10000/tcp --permanent $ firewall-cmd --reload  ","categories": ["Linux","RHEL"],
        "tags": [],
        "url": "http://localhost:4000/linux/rhel/things-to-do-after-installing-rhel-7/",
        "teaser":"http://localhost:4000/assets/images/blog/things-to-do-after-installing-rhel-7.png"},{
        "title": "DevOps Tool : Docker",
        "excerpt":"   Introduction   Docker is a tool used to perform operating system level virtualization (a.k.a containerization). The docker helps to run software through different containers. Many companies like Paypal, VISA and many more are using Docker to run their products. I first encountered docker when I was exploring DevOps and the tools under DevOps. Docker got my attention and I started learning docker and guess what? I fell in love with it. The concept of containerization and how docker helped me to create containers very easily really fascinated me.   Why Docker   I just conveyed what docker is? and what is the purpose? Now we will discuss, why docker? The main issue that a Developer will face during his career is, the code not working in production server. But, it works just fine in their system. This issue is called environment issue. This issue is due to the difference in the configuration in the developer’s PC and the server. Docker solves this issue by making a virtual environment with all the necessary dependencies. This step helps to solve the dependency issue. The other advantage of docker is the easy implementation of micro-services. Micro-services are the processes that communicate through a network. The communication is basically client-server communication (but done in small chunks).   Why implement Mirco-services?   The main advantage of implementing micro-services is the easy maintainability. Implementing micro-services helps to break the application into different chunks or pieces. These chunks or pieces are called services. These services are built in such a way that they have maximum cohesion and least coupling. Cohesion is the communication of different modules inside the service and coupling is the communication between 2 services. So, each service can be assigned to a different developer. As each developer assigns different dependencies ( depending on the application’s need, personal preference and many more reasons), the docker file can incorporate these dependencies easily. Before using Docker to implement micro-services, virtual machines where used. A virtual machine is an emulation of an actual PC. We can create virtual systems and install operating systems to run it. So to setup micro-services, the team had to create virtual machines (VM) and install operating systems and then run the service. That is, each service will be running on different virtual machines. Virtual machines can be made using applications like VirtualBox, VMware and many more. But, the best application for using virtual machines is KVM (Kernel Virtual Machine). But, KVM is only available to Linux. The main disadvantages of using VMs is that the developer has to specify the memory, processor count and disk space. The virtual machine will use these resource from the host machine even if the VM doesn’t need it. The other disadvantage of using virtual machines is that it becomes impractical when working on a large scale implementation. On the other hand, using Docker helps in easy implement of micro-services. Each docker container can be easily programmed to be a service. Docker doesn’t require any preallocation of memory, the container will automatically allocate the necessary resources dynamically.   What is a Docker?   Docker is a simple tool that has been designed in such a way that it can create, deploy and run applications or services using containers. It is considered as a lightweight alternative to VMs. The docker requires a host machine to work and can allocate its RAM dynamically.   Advantages of Docker      Rapid application development:  The docker containers require fewer runtime resources and require the least storage. Due to this form factor, they are very easy to deploy.   Portability: The services can be easily deployed in any servers via docker.   Version control: Just like Git, Docker container is version controlled. The developer can track the versions of Docker container (service). As Docker has version control, the developer can inspect changes in the file and can rollback changes.   Sharing: You can share your docker file with others through DockerHub. It is a remote repository to store docker files.   Secure: If the docker file fails, it won’t affect the whole application. That is, a failed docker won’t crash the server.   How to use Docker work?      A developer will develop a service and convert the service into a container using Docker. This process is done by creating a docker file, which in turn becomes an image. The developer will then add the dependencies for this service by creating or adding docker images to the file. Eg: For running a web-based application, the developer will add Nginx server docker image and PHP docker image. When the developer runs the docker file, these images work together to run the program. The docker image when run, generates a docker container. After the successful running of the docker container, the developer can push the docker image to DockerHub. He can then pull the docker image remotely from any computer and run the docker file there. If the developer had developed a docker file for the production server, he can easily pull the image from DockerHub and run the image in the server.   Docker Components      Docker Registry: Docker registry is the storage for all the docker images. These images can be public or private. The registry can be cloud-based or local. DockerHub is a cloud-based Docker registry.   Docker image: A Docker image is a read-only template to create Docker containers. Docker image is built in such a way that it incorporates all the dependencies of the developed application.   Docker container: A docker container is a running instance of a Docker image. A container can be a combination of more than one docker image.   Docker Bundle   The docker application consists of 4 software:-     Docker engine: Docker engine creates and runs Docker containers.   Docker CLI Client: Docker CLI client is the command-line interface to interact with Docker.   Docker Compose: Docker compose is used to define and run multi-container Docker applications.   Docker Machine: Docker machine is an application that lets the developer install docker engine on virtual hosts and helps the developer to manage the virtual hosts.   Docker Editions   There are two Docker editions:-     Docker CE (Community Edition): Docker CE is for the developers who have started learning docker.   Docker EE (Enterprise Edition): Docker EE is for enterprise level development.   Docker Installation   Mac      Download Docker for Mac using this link.   Install Docker from dmg.   After installation, open docker from applications.   After opening, the docker whale will be available in Mac’s status bar.   Open ‘about Docker Desktop’  to know the docker engine’s version and other relevant information.      Ubuntu / Debian      First, we will update the repositories.     $ sudo apt-get update           We will install the dependencies for Docker to work.     $ sudo apt install linux-image-extra-$(uname -r ) linux-image-extra-virtual -y           We will now install docker on the machine.     $ sudo apt-get install docker-engine           After installation, we have to start docker service.     $ sudo service docker start           Fedora      Install dnf-plugins-core to manage Fedora’s repositories.     $ sudo dnf -y install dnf-plugins-core           Add Docker repository to the machine.     $ sudo dnf config-manager \\  --add-repo \\  https://download.docker.com/linux/fedora/docker-ce.repo           Install Docker-CE     $ sudo dnf install docker-ce           Start docker service.     $ sudo systemctl enable docker           To start, stop, restart and get status about Docker, use the respective commands.     $ sudo systemctl start docker.service ## &lt;-- Start docker ## $ sudo systemctl stop docker.service ## &lt;-- Stop docker ## $ sudo systemctl restart docker.service ## &lt;-- Restart docker ## $ sudo systemctl status docker.service ## &lt;-- Get status of docker ##           RHEL / CentOS      Docker repo is already available in yum repository.     $ sudo yum install docker           Enable Docker service.     $ sudo systemctl enable docker.service           To start, stop, restart and get status about Docker, use the respective commands.     $ sudo systemctl start docker.service ## &lt;-- Start docker ## $ sudo systemctl stop docker.service ## &lt;-- Stop docker ## $ sudo systemctl restart docker.service ## &lt;-- Restart docker ## $ sudo systemctl status docker.service ## &lt;-- Get status of docker ##           Working with Docker      Pulling an image.     sudo docker pull docker-image           Run a Docker image.     sudo docker run docker-image           View Docker images.     sudo docker ps -as          or      sudo container ls           Remove Docker containers     sudo docker rm image-name           Remove Docker image     sudo docker image rm image-name          ","categories": ["DevOps","Docker"],
        "tags": [],
        "url": "http://localhost:4000/devops/docker/devops-tool-docker/",
        "teaser":"http://localhost:4000/assets/images/blog/devops-tool-docker/devops-tool-docker.png"},{
        "title": "PHP and MySQL Installation and Why no need for Web Server",
        "excerpt":"   Why setup localhost   PHP: Hypertext Pre-Processor is an open-source server-side scripting language used for web development. If you are reading this blog, it’s pretty evident that you know how to use PHP and you are now trying to install PHP in your computer. As PHP is a server-side scripting language, the developer can run any PHP code via the browser.   Usual PHP installation   Usually, for working on PHP, the developer has to install:-     Web Server   MySQL (if you are using database)   PHP   Types of Installation   Depending on your operating system, the installatio can be classified into:-     MAMP Server (Mac, Apache2, MySQL, PHP)   LAMP Server (Linux, Apache2, MySQL, PHP)   WAMP Server (Windows, Apache2, MySQL, PHP)   X-AMPP Server (Any Operating system, Apache2, MySQL, PHP, Perl)   Why Install Web Server or Why not Install Web Server   Web server is required if you are setting up a server. If you are setting up for development. You can run your PHP code using the following command.   $ php -S 0.0.0.0:8080   After executing the command, you can open the address in your browser to open your site.   Installing MAMP Server   I have already written a blog on how to setup a server on Mac in this link. Just skip install web server (Apache) (If for setting up localhost). There is another way to install a MAMP server. There is a application named MAMP server that can be used to activate web server on a click. To install, goto this link. Just install the .dmg and follow the instructions to install the MAMP server.   Installing LAMP Server   Fedora      First, we will update the system.     $ sudo yum update           Let’s install web server (Dont install if for localhost).     $ sudo yum install httpd           Let’s start the httpd service.     $ sudo service httpd start           Now, we will install MySQL for working on databases.     $ sudo yum install mysql mysql-server           Now, we will start MySQL service.     $ sudo service mysqld start           Now, we have to setup root password for MySQL.     $ sudo /usr/bin/mysql_secure_installation           Let’s install PHP.     $ sudo yum install php php-mysql           Now, we have to configure the server and MySQL service to start automatically after every boot. If for localhost, don’t configure the server. Just configure MySQL.     $ sudo chkconfig httpd on $ sudo chkconfig mysqld on           For server, to check, we will create a PHP file in web home directory.     $ sudo nano /var/www/html/info.php           To check if PHP is working, we will run phpinfo().     $ sudo echo \"&lt;?php phpinfo(); ?&gt;\" &gt;&gt; /var/www/html/info.php                Open browser and goto localhost/info.php. If PHP not working, you will get a blank page or the php code will be visible.       If you want to use phpmyadmin, install phpmyadmin by using the following command. If you are using phpmyadmin, then you have to install web server.     $ sudo dnf -y install phpMyAdmin php-mysqlnd php-mcrypt php-php-gettext           Restart apache server.     $ sudo systemctl restart httpd           RHEL / CentOS      Update the system.     $ sudo yum update           Install web server.     $ sudo yum install httpd           Add firewall exception to HTTP service.     $ firewall-cmd --permanent --add-service=http           Now, we have to restart firewall to implement changes.     $ systemctl restart firewalld           Now, we open port 80 in firewall.     $ firewall-cmd —add-port=80/tcp           Now, we will install PHP.     $ sudo yum install php php-mysql php-pdo php-gd php-mbstring           We will check if PHP is working by running phpinfo().     $ echo \"&lt;?php phpinfo(); ?&gt;\" &gt;&gt; /var/www/html/info.php           Now check if PHP is working by opening localhost/info.php in your browser.       Now, we will install mariaDB for database.     $ sudo yum install mariadb-server mariadb           Start the database service.     $ sudo systemctl start mariadb           Now, we have to setup root password for MySQL.     $ mysql_secure_installation           To install phpmyadmin.     $ sudo yum install http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.3-1.el7.rf.x86_64.rpm $ sudo yum install phpmyadmin           We have to edit phpmyadmin configuration file.     $ nano /etc/httpd/conf.d/phpmyadmin.conf           Remove # (comment) from the following lines inside the file.     Order Deny,Allow Deny from all Allow from 127.0.0.1           Now we have to enable httpd and mariadb so that they will start automatically after every boot.     $ systemctl enable mariadb $ systemctl enable httpd           Installing WAMPSERVER      Download Wampserver using this link.   Open installation file and follow the steps to install it to the system.   Open wampserver to enable webserver, database and PHP.   To check if wampserver is working, goto http://localhost in the browser.   Installing XAMPP server      Download XAMPP using this link. Download the installation file for your operating system.   Install it by following the instructions.   Open XAMPP to enable apache, mysql and php.  ","categories": ["Web"],
        "tags": [],
        "url": "http://localhost:4000/web/PHP-installation/",
        "teaser":"http://localhost:4000/assets/images/blog/setting-Up-localhost.jpg"},{
        "title": "Simple Home Automation using Pi Zero W, Particle.io, Google assistant and IFTTT",
        "excerpt":"   Introduction   Almost everyone’s dream is to have an automated home. But, automated home is expensive and developing on your own is a major task until now. Today we will make simple home automation device using Raspberry Pi Zero W that can automate a switch using relays.   Hardware Requirements      Raspberry Pi Zero W - 1   2 - Channel relay   Some wires   Software Requirements      Particle.io account   IFTTT application   Things to know before the build   Particle.io Pin diagram   The pin numbering of particle.io on Raspberry Pi is different from the standard one. The below diagram will explain.      Instructions   1. Initial step      Log in to github and to this link and fork the github repository.   Download the repository to your computer.   2. Particle.io           Goto to particle.io webIDE using this link.             Add title and copy the code from the downloaded repository (present in code/main_program/main_program.ino) and paste it on the code editor.            Click on the compile ( present at left, above the folder symbol).       3. Setup Raspberry Pi Zero W      Download etcher and install it on your system.   Download raspberry pi stretch image.   Flash the image to an SD card via the etcher.   Remove and insert sdcard into the system.   Create a file ssh inside boot.   Create another file named wpa_supplicant.conf inside boot and add the following code to insert Wi-Fi credentials.     ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev network={  ssid=\"YOUR_NETWORK_NAME\"  psk=\"YOUR_PASSWORD\"  key_mgmt=WPA-PSK }          Replace YOUR_NETWORK_NAME with your WiFi’s ssid and YOUR_PASSWORD with it’s password.       Now, insert the sdcard inside the Raspberry Pi Zero W and fire up the Pi.   Find the Raspberry Pi’s IP address. You can do that by using applications like Fing.   now ssh to the Pi using your Linux or Mac system. If you are using Windows ( First of all goto hell because you are using Windows), then download putty to ssh to the Pi.   To SSH into the Pi.     $ ssh pi@ipaddress           Now, update and upgrade the Pi.     $ sudo apt update &amp;&amp; apt upgrade           Install particle.io client into the Pi.     $ bash &lt;( curl -sL https://particle.io/install-pi )          You will be asked to enter your particle.io username and password. Also add the name of the Pi. Now the Pi is setup.       Now upload the code to the Pi (Present on the left side of the WebIDE).    4. Connecting relay with Raspberry Pi Zero W      Connect D0 of Raspberry Pi (Refer pin diagram from above) to relay 1.   Connect D1 of Raspberry Pi to relay 2.   Connect 5v for relay’s power.   Connect GND of Pi to GND of the 2 relays.      5. Setup IFTTT      Click ‘+’ from the app.    Click ‘this’.    Search google assistant and click on it.    Click ‘say a simple phrase’.    Specify the credentials as below.    Click ‘that’.    search and select ‘particle.io’.    Specify the function name.    Repeat the same for the other functions functions like bulboff, tubelight on and tubelight off.   It’s done. Now command your google assistant to turn on tubelight and see the wonder. Watch the following video for demo.     ","categories": ["Raspberry Pi","Home Automation"],
        "tags": [],
        "url": "http://localhost:4000/raspberry%20pi/home%20automation/simple-home-automation/",
        "teaser":"http://localhost:4000/assets/images/blog/simple-home-automation/simple-home-automation.png"},{
        "title": "Kernel-based Virtual Machine (KVM)",
        "excerpt":"   Introduction   I first installed Windows, when I was in 7th standard. It was Windows XP. That time, I really loved to use different operating systems. But, during those days I didn’t know how to use multiple operating systems as virtual machines until I reached by the 11th standard. That time, I was using Windows 7 ultimate and I wanted to use Windows Vista for knowing the experience and Windows XP for feeling the nostalgia. I started surfing the internet for solutions. One of the solutions was to use VirtualBox. I installed VirtualBox and installed both the operating systems and started using both of them. It was going well at first. But after a few days, I started noticing lags and graphics issues. I went along till my BCA first year. When I got my first Laptop (Lenovo Y50-70), I started searching for alternatives for VirtualBox and came across VMWare. VMWare fixed many of the issues, but it was a trail pack and had to pay money to use it. So, I switched back to VirtualBox. It went on till my MCA. During my first year of MCA, I uninstalled Windows 10 completely and installed Fedora. I installed VirtualBox for Virtualization. I was then suggested by my senior, Ashwin Babu Karuvally that KVM is better than VirtualBox. I started surfing the internet for KVM and I solved many issues that I faced while using VirtualBox.   What is KVM?   Kernel-based Virtual Machine is an open source virtualization technology built into Linux. KVM is an alternative for using VirtualBox or VMWare. KVM helps to convert the Linux host system (Your machine) into a hypervisor that can run multiple operating systems as individual and independent virtual environments. These virtual environments are called guest machines or virtual machines. To understand how KVM works, we need to know what KVM is and how it works?   What is Hypervisor?   The hypervisor can be a software or hardware that can run virtual machines. The hypervisor provides a virtual operating environment for running the operating systems. Using hypervisor, a user can run any operating system on the host system. There are mainly 2 types of hypervisors:-     Type-1: Bare-metal hypervisors   Type-2: Hosted hypervisors      A type-1: Bare-metal hypervisor is a type of hypervisor communicates directly with the host’s hardware. That is, the guest machine’s hardware directly communicates with the host’s machine. This type of hypervisors are fast and have least delay time. When the user triggers a command in guest operating system, it sends the command to the hypervisor and the hypervisor sends the command to the host’s hardware. Kernel-based Virtual Machine (KVM), Red Hat Enterprise Virtualization (RHEV), XenServer, Microsoft Windows Server 2012 Hyper-V and VMware vSphere are examples of this type of hypervisors.   A type-2: Hosted hypervisor is a type of hypervisor communicates using the host operating system. A hosted hypervisor creates each running instance of the guest machine as a process in the host operating system. So for every request from the guest operating system goes to the host operating system and from there to the hardware. This communication takes time to execute each request. These kinds of hypervisors are not efficient in terms of performance. This type of hypervisor is also called hosted hypervisors. Oracle VirtualBox, VMWare Workstation and Windows Virtual PC are examples of this type of hypervisor.   Why KVM     Free and open source software.   Secure as it uses sVirt (Secure virtualization) and SELinux for securing and isolating every guest operating system.   Wide hardware support   Efficient memory management   Scalable   Requirements before installing KVM   Make sure to enable Intel Virtualization Technology (INTEL VT) or AMD-V Technology for Client Virtualization. This setting can be enabled through the system’s BIOS settings.   Installing KVM  Ubuntu 18.04     Lets install KVM and virtual machine manager.     $ sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager           Next we need to find the name of the network interfaces. It will be like enp8s0 or something like that.     $ ip a           Now, we have to edit the network interface’s configuration file to create a network bridge. This bridge will provide internet to all the virtual machines (VM). The file will be empty.     $ sudo nano /etc/network/interfaces           Add the following line.     auto lo br0 iface lo inet loopback iface enp8s0 inet manual iface br0 inet dhcp   bridge_ports enp8s0           Now we will add the user to appropriate group, so that he can open any KVM without need for root access.     $ sudo adduser username libvirt $ sudo adduser username libvirt-qemu           Reboot your system.     $ sudo reboot           Fedora 29      Install KVM     $ sudo dnf install @virtualization           Let’s start the service.     $ sudo systemctl start libvirtd $ sudo systemctl enable libvirtd           Here, no need to setup network bridge because during installation, the installer will automatically do that.   Other Methods to use KVM  If you want to use KVM on your home server. That is, if you have a server that is used for virtualization only. You can use Proxmox server. Proxmox was first suggested by my senior, Tharun P Karun. I used proxmox server for my virtualization server. Learn more about proxmox server from this link.  ","categories": ["Linux","KVM"],
        "tags": [],
        "url": "http://localhost:4000/linux/kvm/intsall-kvm-on-linux/",
        "teaser":"http://localhost:4000/assets/images/blog/kernel-based-virtual-machine/kernel-based-virtual-machine.png"},{
        "title": "Introduction to Git",
        "excerpt":"   Introduction   Git is a distributed version control for tracking changes and to maintain multiple versions of code during the development of a software project. To understand Git, we should understand version control.   Version Control   A version control a.k.a source control a.k.a revision control is an application that helps to maintain versions of a code. Before we continue any further, we must know some important terms that will be used while we study version control:-     Repository: A repository is a place to store the code and other files created and modified during the development of the application. A repository also maintains the history of the code and other files generated during the development of the program.   Trunk: Trunk is the master branch of the git repository. That is, it is the main development line of the repository. All the branches merged will at last merge with this line to form the main line.   Tags: Tag is a descriptive name given to a specific version of the project. It forms as an memorable alternative to the id generated by version control.   Branch: A branch generates multiple development lines for the repository. This feature helps when there are multiple people working on the same project.   pull: Pulling is the process of downloading recent changes from the central repository to the computer’s local repository. If the repository is being downloaded for the first time, it is called cloning.   Pushing: Pushing is the process of uploading all the changes into the central server.   Committing: Committing is the process through which the version control records and stores changes to the central repository. The committing process also helps to document the changes done by the developer.   That is, each change in the program’s code can be recorded during the development of the application. The major advantages of version control are:-     If the developer’s recent change has crashed, he/she can revert back to his previous versions.   A developer can compare different versions of his code for bug fixing, release documentation and many more purposes.   Version control helps in efficient usage team members during development of group projects as multiple developers can contribute to the project. Each developer can track changes done by each developer to the project.   Version control supports branching, a method to create multiple workflow that doesn’t affect the master branch. The developer can also merge these multiple branches to the master line.   Acts as a backup for the code.   A version control works on the principle of graph data structure. That is, each change recorded during commit forms a node in the graph data structure.      The above image is self explanatory on how version’s branching and merging works using graph data structure.   Types of Version Control   The types of version control are:-     Local version control   Centralized version control   Distributed version control   A local version control maintains versions of the code on the local computer. One of the most common methods of maintaining versions is local version control. The versions are maintained as different files. Due to this methodology, there is a great chance for accidental loss of data due to accidental changes to the wrong file.   A centralized version control is a version control approach through which different versions of the code are maintained on a centralized server. A centralized version control can store versions of code and has a list of user’s who has the privilege to access the repository (Central code). An example of centralized version control is Tortoise SVN.   A distributed version control is a type of version control system that helps to clone the repository in which the code is present along with it’s full history. The main advantage of this method is that, if any of the central servers fails, the developer can restore the repository by uploading (pushing) the code to the server. That is, a distributed version control creates a complete backup of the repository into the local computer. An example of distributed version control is git.   The popular version systems are:-     Git   SVN (Apache Subversion)   Mercurial   What is Git?   Git is distributed version control application that comes under General Public License 2 (GPL 2). Git was developed by Linus Torvalds during the development of Linux kernel in 2005. As git comes under distributed version control, the local repository of projects developed using git will have complete history of changes irrespective of internet access or access to the server. Git is developed using C, Python, TCL,Perl and Shell.   Installing Git   Ubuntu / Debian  $ sudo apt install git-all   Fedora  $ sudo dnf install git-all   Centos / RHEL  $ sudo yum install git-all   Mac  You can install git using brew package manager as follows.  $ brew install git  Or, you can install as follows:-     Download git for mac from this link.   Execute the installer and follow the steps to install git into mac.   But, I would prefer to use brew because you can update the application easily by just executing brew update.   Windows      Download git for windows using this link.   Execute the installer and follow the steps to install git into windows.  ","categories": ["DevOps","Git"],
        "tags": [],
        "url": "http://localhost:4000/devops/git/introduction-to-git/",
        "teaser":"http://localhost:4000/assets/images/blog/introduction-to-git/introduction-to-git.png"},{
        "title": "Working with Git",
        "excerpt":"   Git   As we have learned what git and version control is, from my previous blog. Now, we will learn git commands, a developer will use at his early stages. To know the version of the git installed inside the computer, goto terminal and type:-   $ git --version      Now, for using git, you need a service that uses git as version control. The service will act as your central / remote / online repository for your git repositories. You can use any service like github, gitlab, bitbucket and many more. I am using github for my projects. Goto any service and create an account in them. Goto the service and create a repository for learning git.   Now, we will configure the git installed in the computer   $ mkdir test-repo # to create a test repo folder $ cd test-repo # got inside the folder $ git init # to initialise the folder as a git repository      After creating a local repo, we will to link the repository to a service that git like github, gitlab etc. We will create a repository in Gitlab.      Now, link the local repository created to the remote repository created o gitlab or any other server. You can do that by running the following command.   $ git remote add name url     If you see the above image the command executed is git remote add origin https://gitlab.com/Sashuu6/test-repo.git, origin attribute defines the original path of repository from where it is taken. The URL after origin is taken as the repository’s source URL. After this step, we need to set the upstream. That is, you have to set the branch to which you are going to upload the code.   $ git push --set-upstream origin branch-name      Now, if you dont want to do all these steps. That is, creating local repository using git init, add remote link using git remote add &lt;name&gt; &lt;url&gt; and setup upstream using git push --set-upstream origin master. You can just create the repository on github or gitlab and just clone the repository.   You can clone a git repository using:-     URL   SSH      If you want to clone a git repository using URL, you can do that using the following.   $ git clone repo-url      If you clone using URL, everytime you try to push code into the github / gitlab (or any remote repository), the console will ask for username and password. If you dont want that, you can use ssh.   For using SSH, you need to do setup you system. You need to generate keys for communication. You can do that using the following command.   $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"  After executing the above command, you will be prompted just press enter. You might be asked for password, if you are using a public computer, then you should keep it or else no need. You need to copy the key generated to github or gitlab. Enter the following command to view the key.   $ cat id_rsa.pub   The generated ID will be displayed on the console. To validate if it is correct or not, it will start from ssh-rsa. Copy the key and goto github settings (if using github) and goto ‘SSH and GPG Keys’ and click ‘new SSH Key’ and give a title (Your PC hostname) and paste the copied key to the next field and save. Now, we can clone using SSH.   $ ssh clone ssh-url      We have successfully created a git repository. We will now configure the git engine installed in our system. The configuration involves telling who we are. That is, username and user email. We can do that as follows.   $ git config --global user.name \"yourname\" # to set your name $ git config --global user.email \"youremail\" # to set your email     If you want to enable colour highlighting for your git console, just run the following commands.   $ git config --global color.ui true $ git config --global color.status auto $ git config --global color.branch auto     When, we have to write huge commit message, we need a text editor linked to it. For doing that, we use the following command.   $ git config --global core.editor editor-name      Here, if you see, I have given all config commands an tag --global, using this tag applies the configuration to your system and will be automatically implemented to all the git repositories you create inside your computer. If you are using git on your college computer, then from your git repository, run these configuration commands without using --global tag.   Now, we have added new configurations to the git repository. To view the configuration, use the following command.   $ git config --list   Now, we have setup the git repository. Now, we will learn to make commits and push code to remote (Github or Gitlab or any other) repository. Now we have added code to the repository and now we want to make a commit. First, we need to prepare the files that needs to be committed. We call it staging process. You can do that using the following command.   $ git add file-name   The other attributes for git add are * and .. I use * because, it will select all the changes made in current directory and it’s sub folders. If you use ., it will select only the files present on the current directory. It will not select the sub folders.   Now, we will commit the staged files. Use the following command to commit changes.   $ git commit -m \"message\"  Replace message with your message. If you have to write a huge message, just type `git commit’, your prefered editor will pop up and type the message.   As we have commited the code, we need to push it to the remote repository. You can do that using the following code.   $ git push     ","categories": ["DevOps","Git"],
        "tags": [],
        "url": "http://localhost:4000/devops/git/working-with-git/",
        "teaser":"http://localhost:4000/assets/images/blog/working-with-git/working-with-git.jpeg"}]
